{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8458e4a2-d67b-4630-bcb7-36684c617db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance\n",
    "metric in KNN? How might this difference affect the performance of a KNN classifier or regressor? \"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\" Euclidean distance: It is the straight-line distance between two points in Euclidean space. It considers both horizontal and vertical distances and is influenced by diagonal distances.\n",
    "\n",
    "Manhattan distance: Also known as L1 distance or city block distance, it is the sum of the absolute differences between the coordinates of two points. It only considers horizontal and vertical movements.\n",
    "\n",
    "The main difference is in how they measure distance. Euclidean tends to be sensitive to varying scales, while Manhattan is less sensitive since it only accounts for the magnitude of differences. The choice between them depends on the nature of the data. In cases where features are on different scales, Manhattan distance might perform better. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c106a0d-4c94-4de2-85fd-67767c13572d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be\n",
    "used to determine the optimal k value? \"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\" Choosing the optimal value of k involves a trade-off. A small k may lead to noisy predictions, while a large k may smooth out patterns. Techniques to determine the optimal k include cross-validation and grid search. Cross-validation helps assess performance across different k values, and grid search systematically tests various k values to find the one that optimizes performance. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3ade09-78ef-4697-8fd3-a17601b38a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In\n",
    "what situations might you choose one distance metric over the other? \"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\" The choice of distance metric influences how the algorithm measures similarity between data points. Euclidean distance is sensitive to varying scales and is influenced by diagonal distances, while Manhattan distance is less sensitive to scale. The impact on performance depends on the dataset characteristics. If features have different scales, Manhattan distance may perform better. It's common to experiment with different distance metrics to see which works best for a particular dataset. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca602938-58a4-4cbd-9a79-81e3eac1171a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect\n",
    "the performance of the model? How might you go about tuning these hyperparameters to improve\n",
    "model performance? \"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\" Common hyperparameters in KNN include the value of k, the choice of distance metric, and the method used to weight the neighbors (uniform or distance-based weights). Tuning these hyperparameters involves experimenting with different values and combinations using techniques like grid search or randomized search. The impact on performance varies depending on the dataset, and hyperparameter tuning helps find the configuration that optimizes performance. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a98f98c-bdba-4a2d-8f4f-1e5b6cc6671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What\n",
    "techniques can be used to optimize the size of the training set? \"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\" A smaller training set may result in overfitting, as the model relies heavily on local patterns. A larger training set can provide a more robust representation of the data. However, a very large dataset may become computationally expensive. Techniques to optimize the size of the training set include using cross-validation to assess performance with different training set sizes and considering techniques like random sampling or stratified sampling. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71833cfb-e1d1-485a-8a7a-f75790113605",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you\n",
    "overcome these drawbacks to improve the performance of the model?  \"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\" Drawbacks of KNN include sensitivity to irrelevant features, computational expense for large datasets, and degradation of performance in high-dimensional spaces (curse of dimensionality). To overcome these, feature selection or dimensionality reduction techniques can be employed. Additionally, optimizing the algorithm's parameters, such as choosing an appropriate k and using distance metrics that are less sensitive to irrelevant features, can enhance performance. For large datasets, consider approximate nearest neighbor algorithms. \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
